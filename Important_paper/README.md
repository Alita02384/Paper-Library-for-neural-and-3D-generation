# 1. Important Paper for us 

**这个章节包含了几篇对我们研究十分重要的顶会论文，需仔细调研阅读**

- [[CVPR 2025]](Neuro-3D_Towards_3D_Visual_Decoding_from_EEG_Signals_CVPR_2025.pdf) **Neuro-3D: Towards 3D Visual Decoding from EEG Signals** [:octocat:](https://github.com/gzq17/neuro-3D)

  *Guo, Jia and Lu, Shuai and Zhang, Weihang and Chen, Fang and Li, Huiqi and Liao, Hongen*

<div align="center">
  <img src="https://github.com/user-attachments/assets/1ef17bf6-41a6-41c6-8f5d-7361b98f4e81" width="50%">
</div>

<details close>
<summary><b>📋 Abstract (Click to Expand)</b></summary>
Recent studies highlighted a practical setting of unsupervised anomaly detection (UAD) that builds a unified model for multi-class images. Despite various advancements addressing this challenging task, the detection performance under the multi-class setting still lags far behind state-of-the-art class-separated models. Our research aims to bridge this substantial performance gap. In this paper, we present Dinomaly, a minimalist reconstruction-based anomaly detection framework that harnesses pure Transformer architectures without relying on complex designs, additional modules, or specialized tricks. Given this powerful framework consisting of only Attentions and MLPs, we found four simple components that are essential to multi-class anomaly detection: (1) Scalable foundation Transformers that extracts universal and discriminative features, (2) Noisy Bottleneck where pre-existing Dropouts do all the noise injection tricks, (3) Linear Attention that naturally cannot focus, and (4) Loose Reconstruction that does not force layer-to-layer and point-by-point reconstruction. Extensive experiments are conducted across popular anomaly detection benchmarks including MVTec-AD, VisA, and Real-IAD. Our proposed Dinomaly achieves impressive image-level AUROC of 99.6%, 98.7%, and 89.3% on the three datasets respectively, which is not only superior to state-of-the-art multi-class UAD methods, but also achieves the most advanced class-separated UAD records.
</details>

- [[CONFERENCE YEAR]](https://example.com/path-to-paper) **Paper Title Goes Here** [:octocat:](https://github.com/username/repository)

  *Author, First and Author, Second and Author, Third*

<div align="center">
  <img src="https://github.com/user-attachments/assets/your-image-asset-id" width="50%">
</div>

<details close>
<summary><b>📋 Abstract (Click to Expand)</b></summary>
Your abstract content goes here. This section is hidden by default and can be expanded by clicking the summary line above. You can place a detailed summary or key points of the paper inside this block.
</details>