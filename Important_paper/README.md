# 1. Important Paper for us 

**è¿™ä¸ªç« èŠ‚åŒ…å«äº†å‡ ç¯‡å¯¹æˆ‘ä»¬ç ”ç©¶ååˆ†é‡è¦çš„é¡¶ä¼šè®ºæ–‡ï¼Œéœ€ä»”ç»†è°ƒç ”é˜…è¯»**

- [[CVPR 2025] **Neuro-3D: Towards 3D Visual Decoding from EEG Signals**](Neuro-3D_Towards_3D_Visual_Decoding_from_EEG_Signals_CVPR_2025.pdf)
  
  *Guo, Jia and Lu, Shuai and Zhang, Weihang and Chen, Fang and Li, Huiqi and Liao, Hongen*[:octocat:code](https://github.com/gzq17/neuro-3D)

<div align="center">
  <img src="https://github.com/user-attachments/assets/93cfe9fb-49ae-4d40-b1ed-cd12389a0cd8" width="80%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Human's perception of the visual world is shaped by the stereo processing of 3D information. Understanding how the brain perceives and processes 3D visual stimuli in the real world has been a longstanding endeavor in neuroscience. Towards this goal, we introduce a new neuroscience task: decoding 3D visual perception from EEG signals, a neuroimaging technique that enables real-time monitoring of neural dynamics enriched with complex visual cues. To provide the essential benchmark, we first present EEG-3D, a pioneering dataset featuring multimodal analysis data and extensive EEG recordings from 12 subjects viewing 72 categories of 3D objects rendered in both videos and images. Furthermore, we propose Neuro-3D, a 3D visual decoding framework based on EEG signals. This framework adaptively integrates EEG features derived from static and dynamic stimuli to learn complementary and robust neural representations, which are subsequently utilized to recover both the shape and color of 3D objects through the proposed diffusion-based colored point cloud decoder. To the best of our knowledge, we are the first to explore EEG-based 3D visual decoding. Experiments indicate that Neuro-3D not only reconstructs colored 3D objects with high fidelity, but also learns effective neural representations that enable insightful brain region analysis.
</details>

<details close>
<summary><b>ğŸ“‹ å¯¹æ¯”å®éªŒè°ƒç ” </b></summary>
<div align="center">
  <img src="https://github.com/user-attachments/assets/93cfe9fb-49ae-4d40-b1ed-cd12389a0cd8" width="80%">
</div>
</details>

- [[CONFERENCE YEAR] **Paper Title Goes Here**](/path-to-paper)
  
  *Author, First and Author, Second and Author, Third* [:octocat:code](https://github.com/username/repository)

<div align="center">
  <img src="https://github.com/user-attachments/assets/your-image-asset-id" width="50%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Your abstract content goes here. å¯ä»¥å»arivxæœæ–‡ç« åå­—ç„¶åå¤åˆ¶æ‘˜è¦ã€‚
</details>

[![](https://capsule-render.vercel.app/api?type=waving&height=200&color=0:0F172A,65:4F46E5,100:22D3EE&text=Click%20and%20Back%20to%20Content&section=footer&fontSize=30&fontAlignY=65&fontColor=FFFFFF)](../README.md)
