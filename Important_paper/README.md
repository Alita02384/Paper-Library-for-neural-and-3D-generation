# 1. Important Paper for us 

**è¿™ä¸ªç« èŠ‚åŒ…å«äº†å‡ ç¯‡å¯¹æˆ‘ä»¬ç ”ç©¶ååˆ†é‡è¦çš„é¡¶ä¼šè®ºæ–‡ï¼Œéœ€ä»”ç»†è°ƒç ”é˜…è¯»**

## [[CVPR 2025] **Neuro-3D: Towards 3D Visual Decoding from EEG Signals**](Neuro-3D_Towards_3D_Visual_Decoding_from_EEG_Signals_CVPR_2025.pdf)
  
  *Guo, Jia and Lu, Shuai and Zhang, Weihang and Chen, Fang and Li, Huiqi and Liao, Hongen* [:octocat:code](https://github.com/gzq17/neuro-3D)

<div align="center">
  <img src="https://github.com/user-attachments/assets/93cfe9fb-49ae-4d40-b1ed-cd12389a0cd8" width="80%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Human's perception of the visual world is shaped by the stereo processing of 3D information. Understanding how the brain perceives and processes 3D visual stimuli in the real world has been a longstanding endeavor in neuroscience. Towards this goal, we introduce a new neuroscience task: decoding 3D visual perception from EEG signals, a neuroimaging technique that enables real-time monitoring of neural dynamics enriched with complex visual cues. To provide the essential benchmark, we first present EEG-3D, a pioneering dataset featuring multimodal analysis data and extensive EEG recordings from 12 subjects viewing 72 categories of 3D objects rendered in both videos and images. Furthermore, we propose Neuro-3D, a 3D visual decoding framework based on EEG signals. This framework adaptively integrates EEG features derived from static and dynamic stimuli to learn complementary and robust neural representations, which are subsequently utilized to recover both the shape and color of 3D objects through the proposed diffusion-based colored point cloud decoder. To the best of our knowledge, we are the first to explore EEG-based 3D visual decoding. Experiments indicate that Neuro-3D not only reconstructs colored 3D objects with high fidelity, but also learns effective neural representations that enable insightful brain region analysis.
</details>

<details close>
<summary><b>ğŸ“‹ 2025.8.15 å¯¹æ¯”å®éªŒè°ƒç ” cjh</b></summary>
<div align="center">
  <img src="https://github.com/user-attachments/assets/93cfe9fb-49ae-4d40-b1ed-cd12389a0cd8" width="80%">
</div>
</details>

<details close>
<summary><b>ğŸ“‹ ä¸ºä»€ä¹ˆåŠ å…¥fMRIçš„æ•°æ®é›†è¿›è¡Œå¯¹æ¯” cyf </b></summary>
<div align="center">
  <img src="https://github.com/user-attachments/assets/9e839734-68aa-4bb9-8e1b-6fa1a5e3d213" width="80%">
</div>
      è®ºæ–‡é€šè¿‡fMRIé‡å»º3Dçš„ä¸è¶³ä¸ç¼ºé™·å¼•å‡ºEEGé‡å»º3Dçš„ä¼˜ç‚¹ä¸åˆ›æ–°ï¼Œæ‰€ä»¥åœ¨æ•°æ®é›†å¯¹æ¯”æ—¶ä¼šåŠ å…¥ç›¸å…³çš„fMRIæ•°æ®é›†
</details>

## [[CVPR 2025] **NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery**](https://arxiv.org/abs/2506.06898)
<details close>

  *Reese Kneeland, Paul S. Scotti, Ghislain St-Yves, Jesse Breedlove, Kendrick Kay, Thomas Naselaris* [:octocat:code](https://www.naturalscenesdataset.org/)

<div align="center">
  <img src="https://github.com/user-attachments/assets/faf4b8a2-72ba-4aa1-8af3-a18cec0b8076" width="80%">
</div>

<details close>
<summary><b>ğŸ“‹ 2025.8.18 åˆæ¬¡æ€»ç»“ cjh</b></summary>
è¿™ç¯‡è®ºæ–‡å‘å¸ƒäº†ä¸€ä¸ªåä¸º **NSD-Imagery** çš„æ–°åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†è®°å½•äº†äººç±»åœ¨è¿›è¡Œâ€œå¿ƒç†æƒ³è±¡â€æ—¶çš„å¤§è„‘åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ´»åŠ¨ã€‚

**æ ¸å¿ƒå†…å®¹å’Œä¸»è¦å‘ç°ï¼š**

1.  **å¯¹ç°æœ‰æ•°æ®é›†çš„è¡¥å……ï¼š** NSD-Imagery æ˜¯å¯¹ç°æœ‰çš„å¤§å‹æ•°æ®é›† NSD (Natural Scenes Dataset) çš„è¡¥å……ã€‚NSD æ•°æ®é›†è®°å½•çš„æ˜¯äººä»¬åœ¨â€œçœ‹åˆ°â€å®é™…å›¾åƒæ—¶çš„å¤§è„‘ fMRI æ´»åŠ¨ï¼Œè€Œ NSD-Imagery åˆ™ä¸“æ³¨äºâ€œæƒ³è±¡â€å‡ºçš„å›¾åƒã€‚

2.  **æ–°çš„è¯„ä¼°ç»´åº¦ï¼š** æ­¤å‰ï¼ŒåŸºäº NSD æ•°æ®é›†è®­ç»ƒçš„å„ç§å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚ MindEye, Brain Diffuser ç­‰ï¼‰éƒ½åªåœ¨â€œçœ‹åˆ°çš„å›¾åƒâ€é‡å»ºä»»åŠ¡ä¸Šè¿›è¡Œè¿‡è¯„ä¼°ã€‚NSD-Imagery çš„å‘å¸ƒï¼Œä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥é¦–æ¬¡è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨é‡å»ºâ€œå¿ƒç†å›¾åƒâ€æ–¹é¢çš„è¡¨ç°å¦‚ä½•ã€‚

3.  **â€œæƒ³è±¡â€æ¯”â€œçœ‹è§â€æ›´å…·æŒ‘æˆ˜æ€§ï¼š** ä»å¤§è„‘æ´»åŠ¨ä¸­é‡å»ºå¿ƒç†å›¾åƒæ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå¿ƒç†å›¾åƒåœ¨å¤§è„‘ä¸­ç¼–ç çš„ä¿¡å·ä¿¡å™ªæ¯”å’Œç©ºé—´åˆ†è¾¨ç‡éƒ½ç›¸å¯¹è¾ƒä½ã€‚ç„¶è€Œï¼Œèƒ½å¦æˆåŠŸåœ°ä»â€œçœ‹è§â€çš„è§£ç æ¨¡å‹æ³›åŒ–åˆ°â€œæƒ³è±¡â€çš„è§£ç ï¼Œå¯¹äºåŒ»ç–—ã€è„‘æœºæ¥å£ç­‰å®é™…åº”ç”¨è‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œéœ€è¦è§£ç çš„ä¿¡æ¯å¾€å¾€æ˜¯å†…éƒ¨äº§ç”Ÿçš„ï¼ˆå³â€œæƒ³è±¡â€ï¼‰ã€‚

4.  **å…³é”®å‘ç°â€”â€”æ¨¡å‹è¡¨ç°ä¸æ¶æ„æœ‰å…³ï¼š**
    *   ä¸€ä¸ªæ¨¡å‹åœ¨é‡å»ºâ€œçœ‹åˆ°çš„å›¾åƒâ€æ–¹é¢è¡¨ç°å¥½ï¼Œå¹¶ä¸æ„å‘³ç€å®ƒåœ¨é‡å»ºâ€œå¿ƒç†å›¾åƒâ€æ–¹é¢ä¹ŸåŒæ ·å‡ºè‰²ã€‚ä¸¤è€…çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯**ä¸ç›¸å…³**çš„ã€‚
    *   æ¨¡å‹çš„**æ¶æ„é€‰æ‹©**å¯¹æ³›åŒ–åˆ°å¿ƒç†å›¾åƒçš„èƒ½åŠ›æœ‰æ˜¾è‘—å½±å“ã€‚é‡‡ç”¨ç®€å•çº¿æ€§è§£ç æ¶æ„å’Œå¤šæ¨¡æ€ç‰¹å¾è§£ç çš„æ¨¡å‹ï¼Œå…¶æ³›åŒ–èƒ½åŠ›æ›´å¥½ã€‚
    *   ç›¸æ¯”ä¹‹ä¸‹ï¼Œé‚£äº›æ¶æ„å¤æ‚çš„æ¨¡å‹æ›´å®¹æ˜“å¯¹â€œçœ‹åˆ°çš„å›¾åƒâ€è®­ç»ƒæ•°æ®äº§ç”Ÿè¿‡æ‹Ÿåˆï¼Œå¯¼è‡´åœ¨å¤„ç†å¿ƒç†å›¾åƒæ—¶æ•ˆæœä¸ä½³ã€‚

**ç»“è®ºï¼š**

ä½œè€…ä»¬è®¤ä¸ºï¼Œè¦å¼€å‘å‡ºçœŸæ­£å®ç”¨çš„è§†è§‰è§£ç åº”ç”¨ï¼ˆå¦‚è„‘æœºæ¥å£ï¼‰ï¼Œä½¿ç”¨åƒ NSD-Imagery è¿™æ ·çš„å¿ƒç†æ„è±¡æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ˜¯è‡³å…³é‡è¦çš„ã€‚ä»–ä»¬å°† NSD-Imagery ä½œä¸ºä¸€ä¸ªå®è´µçš„èµ„æºï¼Œæ—¨åœ¨æ¨åŠ¨è§†è§‰è§£ç æ–¹æ³•æ›´å¥½åœ°æœåŠ¡äºè¿™ä¸€æœ€ç»ˆç›®æ ‡ã€‚

**åº”ç”¨å‰æ™¯**

*   **åŒ»ç–—è¯Šæ–­ï¼š** å¼€å‘ç”¨äºç²¾ç¥ç–¾ç—…çš„è¯Šæ–­å·¥å…·ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä¸ä¾µå…¥æ€§æˆ–è´Ÿé¢æƒ…ç»ªçš„å¿ƒç†æ„è±¡ç›¸å…³çš„ç–¾ç—…ï¼ˆå¦‚ PTSDã€ç„¦è™‘ç—‡ç­‰ï¼‰ã€‚
*   **æ›¿ä»£æ€§äº¤æµï¼š** ä¸ºé‚£äº›å› å„ç§åŸå› æ— æ³•é€šè¿‡å¸¸è§„æ–¹å¼ï¼ˆå¦‚è¯­è¨€ï¼‰è¿›è¡Œäº¤æµçš„æ‚£è€…æä¾›ä¸€ç§æ–°çš„æ²Ÿé€šæ–¹æ³•ã€‚
*   **æ„è¯†æ£€æµ‹ï¼ˆé‡ç‚¹åº”ç”¨ï¼‰ï¼š**
    *   å…¨çƒæ¯å¹´æœ‰ 5000 ä¸‡äººé­å—åˆ›ä¼¤æ€§è„‘æŸä¼¤ï¼Œå…¶ä¸­çº¦ 15-20% çš„æ‚£è€…å¯èƒ½å¤„äºâ€œéšè”½æ€§æ„è¯†ï¼ˆcovertly consciousï¼‰â€çŠ¶æ€ï¼Œå³å¤–è¡¨çœ‹èµ·æ¥æ²¡æœ‰ååº”ï¼Œä½†å†…å¿ƒä»æœ‰æ„è¯†ã€‚
    *   åˆ©ç”¨è¿™ç§éä¾µå…¥æ€§çš„è„‘æˆåƒæŠ€æœ¯ï¼Œæ¥è§£ç å‡ºæ‚£è€…æƒ³è±¡çš„å¯éªŒè¯å†…å®¹ï¼ˆæ¯”å¦‚é—®ä¸€ä¸ªåªæœ‰ä»–çŸ¥é“ç­”æ¡ˆçš„é—®é¢˜ï¼Œè®©ä»–æƒ³è±¡ç­”æ¡ˆï¼‰ï¼Œå¯ä»¥å¸®åŠ©å‡†ç¡®è¯Šæ–­è¿™ç±»æ‚£è€…ï¼Œä»è€Œ**é˜²æ­¢ç”Ÿå‘½æ”¯æŒç³»ç»Ÿè¢«è¿‡æ—©åœ°æ’¤é™¤**ã€‚
</details>

- [[CVPR 2023] **High-resolution image reconstruction with latent diffusion models from human brain activity**](High-resolution_image_reconstruction_with_latent_diffusion_models_from_human_brain_activity.pdf)
  
  *Yu Takagi, Shinji Nishimoto* [:octocat:code](https://sites.google.com/view/stablediffusion-with-brain/)

<div align="center">
  <img src="https://github.com/user-attachments/assets/edf9a662-df79-4441-80d0-81faef41000b" width="40%">
</div>

<details close>
<summary><b>ğŸ“‹ Abstract (Click to Expand)</b></summary>
Reconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector of image Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straight-forward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs.
</details>

- [[CONFERENCE YEAR] **Paper Title Goes Here**](/path-to-paper)

## [[CVPR 2025] **Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior**](https://arxiv.org/abs/2503.04207)
  
  *Haitao Wu, Qing Li, Changqing Zhang, Zhen He, Xiaomin Ying* [:octocat:code](https://github.com/HaitaoWuTJU/Uncertainty-aware-Blur-Prior)

<div align="center">
  <img src="https://github.com/user-attachments/assets/091c9927-e982-4871-a8a9-26f5b5b1ad2e" width="80%">
</div>

<details close>
<summary><b>ğŸ“‹ 2025.8.18 åˆæ¬¡é˜…è¯» cjh</b></summary>
è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³éå¸¸å·§å¦™ï¼Œå®ƒæ¢è®¨å¹¶è§£å†³äº†ä¸€ä¸ªåœ¨â€œä»è„‘ä¿¡å·è§£ç å›¾åƒâ€ä»»åŠ¡ä¸­çš„å…³é”®é—®é¢˜ï¼š**æˆ‘ä»¬çš„å¤§è„‘ä¿¡å·å¹¶ä¸å®Œç¾ï¼Œå……æ»¡äº†ä¿¡æ¯æŸå¤±å’Œå™ªå£°ï¼Œç›´æ¥ç”¨å®ƒæ¥å¯¹é½æ¸…æ™°çš„åŸå§‹å›¾åƒä¼šè®©æ¨¡å‹â€œå­¦å¾—å¾ˆç—›è‹¦â€ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚**

---

### **æ ¸å¿ƒæ‘˜è¦**

ä¸ºäº†è§£å†³è„‘ä¿¡å·ä¸åŸå§‹è§†è§‰å›¾åƒä¹‹é—´çš„â€œä¿çœŸåº¦å·®è·â€ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„æ–°æ–¹æ³•ï¼Œåä¸º**â€œä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡ç³Šå…ˆéªŒâ€ï¼ˆUncertainty-aware Blur Prior, UBPï¼‰**ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ï¼š**åœ¨è®­ç»ƒæ—¶ï¼Œä¸»åŠ¨åœ°ã€æœ‰ç­–ç•¥åœ°å°†åŸå§‹å›¾åƒè¿›è¡Œæ¨¡ç³Šå¤„ç†ï¼Œä½¿å…¶åœ¨ä¿¡æ¯å±‚é¢ä¸Šæ›´æ¥è¿‘äºâ€œä¸å®Œæ•´â€çš„è„‘ä¿¡å·**ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹ä¸å†éœ€è¦è´¹åŠ›åœ°å»å¼¥è¡¥ä¸€ä¸ªå·¨å¤§çš„ä¿¡æ¯é¸¿æ²Ÿï¼Œä»è€Œèƒ½æ›´å¥½åœ°å­¦ä¹ ä¸¤è€…é—´çš„çœŸå®å¯¹åº”å…³ç³»ï¼Œæ˜¾è‘—æå‡äº†è§£ç çš„å‡†ç¡®æ€§ã€‚

---

### **è¯¦ç»†åˆ†è§£**

1.  **æå‡ºäº†ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼ˆä¸¤ä¸ªGAPï¼‰ï¼š**
    *   **ç³»ç»Ÿæ€§å·®è· (System GAP):** å½“æˆ‘ä»¬çœ‹ä¸€ä¸ªä¸œè¥¿æ—¶ï¼Œç”±äºäººç±»è§†è§‰ç³»ç»Ÿçš„ç‰©ç†é™åˆ¶ï¼ˆæ¯”å¦‚æ³¨æ„åŠ›å’Œè®°å¿†åŠ›æœ‰é™ï¼‰ï¼Œä¸€äº›ä¿¡æ¯åœ¨è½¬åŒ–ä¸ºè„‘ä¿¡å·çš„è¿‡ç¨‹ä¸­å°±**å¿…ç„¶ä¸¢å¤±**äº†ã€‚ä¾‹å¦‚ï¼Œå›¾åƒä¸­éå¸¸ç²¾ç»†çš„ã€é«˜é¢‘çš„ç»†èŠ‚å¯èƒ½æ— æ³•è¢«å¤§è„‘å®Œå…¨ç¼–ç ã€‚
    *   **éšæœºæ€§å·®è· (Random GAP):** é™¤äº†ç³»ç»Ÿæ€§çš„ä¿¡æ¯æŸå¤±ï¼Œè¿˜æœ‰å¾ˆå¤š**éšæœºå› ç´ **ä¼šè¿›ä¸€æ­¥é™ä½è„‘ä¿¡å·çš„è´¨é‡ï¼Œæ¯”å¦‚æˆ‘ä»¬çœ‹ä¸œè¥¿æ—¶çš„è®¤çŸ¥çŠ¶æ€æ³¢åŠ¨ã€æµ‹é‡è®¾å¤‡æœ¬èº«å¸¦æ¥çš„æŠ€æœ¯å™ªå£°ç­‰ã€‚

2.  **æŒ‡å‡ºäº†å½“å‰æ–¹æ³•çš„å›°å¢ƒï¼š**
    *   ä¼ ç»Ÿçš„è§£ç æ¨¡å‹è¯•å›¾ç›´æ¥å°†â€œä¸å®Œç¾çš„â€è„‘ä¿¡å·ä¸â€œå®Œç¾çš„â€é«˜æ¸…å›¾åƒè¿›è¡Œé…å¯¹å­¦ä¹ ã€‚
    *   ç”±äºä¸Šè¿°ä¸¤ä¸ªâ€œå·®è·â€çš„å­˜åœ¨ï¼Œè¿™ç§é…å¯¹å®é™…ä¸Šæ˜¯**ä¸åŒ¹é…**çš„ã€‚
    *   åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¾ˆéš¾å­¦ä¼šå¦‚ä½•è·¨è¶Šè¿™ä¸ªå·¨å¤§çš„ä¿¡æ¯é¸¿æ²Ÿï¼Œæœ€ç»ˆå¯¼è‡´**è¿‡æ‹Ÿåˆ**â€”â€”æ¨¡å‹åªæ˜¯æ­»è®°ç¡¬èƒŒäº†è®­ç»ƒæ ·æœ¬ï¼Œè€Œå¯¹äºæ–°çš„ã€æ²¡è§è¿‡çš„è„‘ä¿¡å·ï¼Œå…¶æ³›åŒ–èƒ½åŠ›ä¼šå¾ˆå·®ã€‚

3.  **æå‡ºäº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆ (UBP):**
    *   **æ ¸å¿ƒæ€è·¯ï¼š** ä¸å…¶å¼ºè¿«æ¨¡å‹å»å¼¥è¡¥è¿™ä¸ªå·®è·ï¼Œä¸å¦‚ä¸»åŠ¨æŠŠè¿™ä¸ªå·®è·â€œå¡«å¹³â€ä¸€éƒ¨åˆ†ã€‚å…·ä½“åšæ³•æ˜¯ï¼Œ**é™ä½å®Œç¾å›¾åƒçš„è´¨é‡ï¼Œè®©å®ƒå‘ä¸å®Œç¾çš„è„‘ä¿¡å·â€œé æ‹¢â€**ã€‚
    *   **å…·ä½“æ–¹æ³•ï¼š**
        1.  é¦–å…ˆï¼Œæ¨¡å‹ä¼š**è¯„ä¼°**æ¯ä¸€å¯¹â€œè„‘ä¿¡å·-å›¾åƒâ€æ•°æ®ä¹‹é—´çš„**ä¸ç¡®å®šæ€§ï¼ˆuncertaintyï¼‰**ã€‚è¿™ç§ä¸ç¡®å®šæ€§çš„å¤§å°ï¼Œå°±åæ˜ äº†è¿™å¯¹æ•°æ®ä¹‹é—´çš„â€œå·®è·â€æœ‰å¤šå¤§ã€‚
        2.  ç„¶åï¼ŒåŸºäºè¿™ä¸ªè¯„ä¼°å‡ºçš„ä¸ç¡®å®šæ€§ç¨‹åº¦ï¼ŒUBPä¼š**åŠ¨æ€åœ°**å¯¹åŸå§‹å›¾åƒè¿›è¡Œ**é«˜æ–¯æ¨¡ç³Š**å¤„ç†ã€‚
        3.  **å·®è·å¤§ï¼ˆä¸ç¡®å®šæ€§é«˜ï¼‰** -> **æ¨¡ç³Šç¨‹åº¦å°±é«˜**ï¼Œä¸¢æ‰æ›´å¤šé«˜é¢‘ç»†èŠ‚ã€‚
        4.  **å·®è·å°ï¼ˆä¸ç¡®å®šæ€§ä½ï¼‰** -> **æ¨¡ç³Šç¨‹åº¦å°±ä½**ï¼Œä¿ç•™æ›´å¤šå›¾åƒç»†èŠ‚ã€‚
    *   **æœ€ç»ˆæ•ˆæœï¼š** é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡ç³Šåçš„å›¾åƒåœ¨ä¿¡æ¯å†…å®¹ä¸Šä¸è„‘ä¿¡å·æ›´åŠ åŒ¹é…ï¼Œé™ä½äº†æ¨¡å‹å­¦ä¹ å¯¹é½çš„éš¾åº¦ï¼Œä»è€Œæ”¹å–„äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

4.  **å±•ç¤ºäº†å“è¶Šçš„æˆæœï¼š**
    *   è¯¥æ–¹æ³•åœ¨â€œé›¶æ ·æœ¬è„‘ä¿¡å·åˆ°å›¾åƒæ£€ç´¢â€ä»»åŠ¡ä¸Šå–å¾—äº†å½“å‰æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æˆæœã€‚
    *   **Top-1 å‡†ç¡®ç‡è¾¾åˆ° 50.9%**ï¼ˆæ¯”ä¹‹å‰æœ€å¥½çš„æ–¹æ³•é«˜äº†13.7ä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚
    *   **Top-5 å‡†ç¡®ç‡è¾¾åˆ° 79.7%**ï¼ˆæ¯”ä¹‹å‰æœ€å¥½çš„æ–¹æ³•é«˜äº†9.8ä¸ªç™¾åˆ†ç‚¹ï¼‰ã€‚
    *   è¿™æ˜¯ä¸€ä¸ªéå¸¸æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

<div align="center">
  <img src="https://github.com/user-attachments/assets/dbb5e2b6-f176-42ca-b684-24503a986f61" width="80%">
</div>

**ä¸€å¥è¯æ€»ç»“ï¼š** è¿™ç¯‡è®ºæ–‡ä¸å†å¼ºæ±‚æ¨¡å‹å»å­¦ä¹ ä¸€ä¸ªä»â€œå™ªå£°ä¿¡å·â€åˆ°â€œé«˜æ¸…å›¾åƒâ€çš„ä¸å¯èƒ½ä»»åŠ¡ï¼Œè€Œæ˜¯é€šè¿‡æ™ºèƒ½åœ°â€œæ¨¡ç³Šâ€é«˜æ¸…å›¾åƒï¼Œåˆ›é€ äº†ä¸€ä¸ªä»â€œå™ªå£°ä¿¡å·â€åˆ°â€œæ¨¡ç³Šå›¾åƒâ€çš„æ›´ç®€å•çš„ä»»åŠ¡ï¼Œç»“æœæ•ˆæœæ‹”ç¾¤ã€‚
</details>

[![](https://capsule-render.vercel.app/api?type=waving&height=200&color=0:0F172A,65:4F46E5,100:22D3EE&text=Click%20and%20Back%20to%20Content&section=footer&fontSize=30&fontAlignY=65&fontColor=FFFFFF)](../README.md)
